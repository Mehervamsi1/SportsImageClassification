{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nywaJ2fSSsjE",
   "metadata": {
    "id": "nywaJ2fSSsjE"
   },
   "source": [
    "# **Sports Image Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7K2Q6CX32JN",
   "metadata": {
    "id": "c7K2Q6CX32JN"
   },
   "source": [
    "# I. Dataset Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pwlI8ZRiSpmj",
   "metadata": {
    "id": "pwlI8ZRiSpmj"
   },
   "outputs": [],
   "source": [
    "#!pip install -q kaggle #As the dataset is in the kaggle this command helps installing the API of kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Lsd2zCh_3lZT",
   "metadata": {
    "id": "Lsd2zCh_3lZT"
   },
   "source": [
    "\n",
    "Dataset Collection\n",
    "\n",
    "Installing the Necessary Libraries\n",
    "\n",
    "Dataset Overview\n",
    "\n",
    "Visualizations\n",
    "\n",
    "Data Cleaning and Preprocessing\n",
    "\n",
    "Modelling Techniques\n",
    "\n",
    "Comparison of Model Perfrmances\n",
    "\n",
    "Predections on New Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-TWrUSrJmebQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-TWrUSrJmebQ",
    "outputId": "ad4e578a-0a53-468d-d642-35cb497806c3"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rg0K_D5VMF78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rg0K_D5VMF78",
    "outputId": "7684fd5b-33fa-4915-874a-e384ce2202c8"
   },
   "outputs": [],
   "source": [
    "#!cp /content/gdrive/MyDrive/path/to/kaggle.json /root/.kaggle/kaggle.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LegemAV6T1ky",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "LegemAV6T1ky",
    "outputId": "9e130174-6d2a-47e6-9c02-84c5eee84e61"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "\n",
    "#Upload Kaggle API key\n",
    "#uploaded = files.upload()\n",
    "#uploaded = \n",
    "\n",
    "# import json\n",
    "# with open('kaggle.json') as f:\n",
    "#     data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K9gBOEiGVEy_",
   "metadata": {
    "id": "K9gBOEiGVEy_"
   },
   "outputs": [],
   "source": [
    "# !mkdir -p /root/.kaggle\n",
    "# !cp kaggle.json /root/.kaggle/\n",
    "# #!cp /content/drive/MyDrive/Colab Notebooks/AdvancedPython -Image Classification/kaggle.json /root/.kaggle/kaggle.json\n",
    "# !chmod 600 /root/.kaggle/kaggle.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B5pcm7tJVJGo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5pcm7tJVJGo",
    "outputId": "7bfc9f9d-1133-4d27-ea97-63f19042a3a8"
   },
   "outputs": [],
   "source": [
    "# # Replace 'gpiosenka/sports-classification' with your Kaggle username and dataset name\n",
    "# !kaggle datasets download -d gpiosenka/sports-classification\n",
    "\n",
    "# # Unzip the dataset\n",
    "# !unzip -q sports-classification.zip -d /content/dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XPJV52o2TGbm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPJV52o2TGbm",
    "outputId": "939b8f00-6fd6-47f9-f441-62f559699c9f"
   },
   "outputs": [],
   "source": [
    "\n",
    "# !kaggle datasets download -d gpiosenka/sports-classification #This is the username and dataset. API Key from the Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b491b399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "JTBcN2cRIc9z",
   "metadata": {
    "id": "JTBcN2cRIc9z"
   },
   "source": [
    "Installing plotly for interactive plots in the following Project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ezsN4H04UrM",
   "metadata": {
    "id": "4ezsN4H04UrM"
   },
   "source": [
    "# II. Installing and Importing the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KDhzIvplENIq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KDhzIvplENIq",
    "outputId": "2b345b7c-d05b-4f77-ae1b-278acb434fd5"
   },
   "outputs": [],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6da0935",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow-metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "So8v4c_xhDie",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "So8v4c_xhDie",
    "outputId": "437a6dd7-5082-4dfc-b272-85806d9ed2b2"
   },
   "outputs": [],
   "source": [
    "#!sudo apt-get install tesseract-ocr\n",
    "!pip install pytesseract\n",
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606f0244",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfcfa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install plotly --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af51b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "_jC56WfpBe4K",
   "metadata": {
    "id": "_jC56WfpBe4K"
   },
   "outputs": [],
   "source": [
    "# Importing all the necessary libraries for the project.\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from io import BytesIO\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "import plotly.io as pio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pio.renderers.default = 'browser'\n",
    "pio.renderers.default = 'iframe_connected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e4c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bWNYd2dKfaGq",
   "metadata": {
    "id": "bWNYd2dKfaGq"
   },
   "source": [
    "##  IV. Elementary Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vhteo3z7IxfI",
   "metadata": {
    "id": "vhteo3z7IxfI"
   },
   "source": [
    "**Loading images form the directories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kcSTK6WyTUSP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kcSTK6WyTUSP",
    "outputId": "03518f5a-1971-47c3-ec4e-e08ada2683a4"
   },
   "outputs": [],
   "source": [
    "# As the dataset is organized into train, test, and valid directories\n",
    "train_path = r'/Users/sarveswararaopatchipulusu/Desktop/advanced python class/archive 2/train'\n",
    "test_path = r'/Users/sarveswararaopatchipulusu/Desktop/advanced python class/archive 2/test'\n",
    "valid_path = r'/Users/sarveswararaopatchipulusu/Desktop/advanced python class/archive 2/valid'\n",
    "\n",
    "# As the CSV file is in the dataset directory\n",
    "csv_file_path = r'/Users/sarveswararaopatchipulusu/Desktop/advanced python class/archive 2/sports.csv'\n",
    "\n",
    "# List files in the directories\n",
    "train_files = os.listdir(train_path)\n",
    "test_files = os.listdir(test_path)\n",
    "valid_files = os.listdir(valid_path)\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Number of training samples:\", len(train_files))\n",
    "print(\"Number of test samples:\", len(test_files))\n",
    "print(\"Number of validation samples:\", len(valid_files))\n",
    "\n",
    "# Display the first few rows of the CSV file\n",
    "print(\"\\nCSV file content:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NY7QNfLtfz-4",
   "metadata": {
    "id": "NY7QNfLtfz-4"
   },
   "source": [
    "The data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h9XEJglYf3oN",
   "metadata": {
    "id": "h9XEJglYf3oN"
   },
   "source": [
    "This code defines a Python function called visualize_individual_samples that, given a dataset path, displays a specified number of individual images from each class in the dataset using Matplotlib. The function iterates through each class, selects a few samples, opens and resizes the images, and then uses Matplotlib to show each image with its corresponding class name as the title. This visualization aids in exploring and understanding the dataset during the exploratory data analysis (EDA) phase of a machine learning project. The function is called with the training set path and a specified image size for display.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cIFEI_kMCW6x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cIFEI_kMCW6x",
    "outputId": "0e21ab2f-73f2-4cac-f466-12f89bea798f"
   },
   "outputs": [],
   "source": [
    "# Function to visualize each image with the class name and dimensions\n",
    "def visualize_individual_samples_with_dimensions(data_path, num_samples_per_class=3, image_size=(64, 64)):\n",
    "    classes = os.listdir(data_path)\n",
    "\n",
    "    for class_name in classes:\n",
    "        class_path = os.path.join(data_path, class_name)\n",
    "        class_samples = os.listdir(class_path)[:num_samples_per_class]\n",
    "\n",
    "        for sample in class_samples:\n",
    "            sample_path = os.path.join(class_path, sample)\n",
    "\n",
    "            # Use matplotlib to display the image\n",
    "            img = Image.open(sample_path)\n",
    "            img = img.resize(image_size)  # Resize the image to the specified size\n",
    "\n",
    "            # Get the dimensions of the image\n",
    "            original_width, original_height = img.size\n",
    "\n",
    "            # Create a figure with two subplots\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "            # Plot the original image\n",
    "            ax1.imshow(img)\n",
    "            ax1.set_title(f\"Class: {class_name}\")\n",
    "            ax1.axis('off')\n",
    "\n",
    "            # Plot the dimensions separately\n",
    "            ax2.text(0.5, 0.5, f\"Dimensions: {original_width} x {original_height}\", ha='center', va='center', fontsize=14)\n",
    "            ax2.axis('off')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "# Visualize each image from the training set with the class name and dimensions\n",
    "visualize_individual_samples_with_dimensions(train_path, image_size=(128, 128))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HURemXzXggkf",
   "metadata": {
    "id": "HURemXzXggkf"
   },
   "source": [
    "The following Python funtions visualize_class_distribution, visualize_class_distribution_pie, visualize_class_distribution_treemap, visualize_class_distribution_scatter use Plotly Express to create an interactive bar plot, pie chart, treemap and scatter plot respeectively showcasing the distribution of samples across different classes in a dataset. These functions takes the dataset path as input, computes the number of samples per class, and generates plots with custom colors and layout settings. The resulting visualization provides insights into the distribution of data, with each unit representing a class and its size indicating the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J17q70-kEGvw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 917
    },
    "id": "J17q70-kEGvw",
    "outputId": "8a64b831-82ae-4e17-bfb2-2b40f3ee1fae"
   },
   "outputs": [],
   "source": [
    "#Function to visualise the distribution on the trainingset as the bar graph.\n",
    "def visualize_class_distribution(data_path):\n",
    "    classes = os.listdir(data_path)\n",
    "\n",
    "    # Filter out non-directory entries (e.g., .DS_Store on macOS)\n",
    "    classes = [class_name for class_name in classes if os.path.isdir(os.path.join(data_path, class_name))]\n",
    "\n",
    "    class_distribution = [len(os.listdir(os.path.join(data_path, class_name))) for class_name in classes]\n",
    "\n",
    "    # Create an interactive bar plot using Plotly Express\n",
    "    fig = px.bar(x=classes, y=class_distribution, labels={'x': 'Class', 'y': 'Number of Samples'},\n",
    "                 title='Class Distribution in Training Set', width=1200, height=900,\n",
    "                 color=classes, color_continuous_scale='Viridis')\n",
    "\n",
    "    # Customize the layout for a professional look\n",
    "    fig.update_layout(\n",
    "        xaxis_title='Class',\n",
    "        yaxis_title='Number of Samples',\n",
    "        showlegend=False,  # Hide the legend for simplicity\n",
    "        margin=dict(l=50, r=50, t=50, b=50),  # Adjust margin for better spacing\n",
    "        plot_bgcolor='lightgray',  # Set plot background color to light gray\n",
    "        paper_bgcolor='white',  # Set paper background color to white\n",
    "        xaxis=dict(showgrid=True, gridwidth=0.5, gridcolor='lightgray'),  # Show x-axis grid\n",
    "        yaxis=dict(showgrid=True, gridwidth=0.5, gridcolor='lightgray'),  # Show y-axis grid\n",
    "    )\n",
    "\n",
    "    # Show the plot interactively\n",
    "    fig.show()\n",
    "\n",
    "# Visualize class distribution interactively in the training set\n",
    "visualize_class_distribution(train_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "katps7MNHU6c",
   "metadata": {
    "id": "katps7MNHU6c"
   },
   "source": [
    "The above plot 'Class distribution in Training Set' shows that the number of samples in the training dataset varies from the number 59 to 191. The 59 image samples are for the class sky surfing and the 191 image samples are for the football."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ZUqV-HkWybn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "1ZUqV-HkWybn",
    "outputId": "5bf9e87d-a34a-45a8-9b00-ab0a71e3157d"
   },
   "outputs": [],
   "source": [
    "#Function to visualise the distribution on the trainingset as the pie chart.\n",
    "def visualize_class_distribution_pie(data_path):\n",
    "    # Get class names from the subdirectories in the data path\n",
    "    classes = [class_name for class_name in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, class_name))]\n",
    "\n",
    "    # Calculate the number of samples for each class\n",
    "    class_distribution = [len(os.listdir(os.path.join(data_path, class_name))) for class_name in classes]\n",
    "\n",
    "    # Create an interactive pie chart using Plotly Express\n",
    "    fig = px.pie(names=classes, values=class_distribution, title='Class Distribution in Training Set using a Pie Chart')\n",
    "\n",
    "    # Update layout for better appearance\n",
    "    fig.update_layout(\n",
    "        autosize=False,  # Turn off autosize for custom size\n",
    "        width=800,  # Set the width of the figure\n",
    "        height=600,  # Set the height of the figure\n",
    "        margin=dict(l=20, r=20, t=40, b=20),  # Adjust margin for better spacing\n",
    "    )\n",
    "\n",
    "    # Update traces (slices) to improve the pie chart layout\n",
    "    fig.update_traces(\n",
    "        pull=[0.1] * len(classes),  # Adjust the pull to increase the size of slices\n",
    "        textinfo='percent+label',  # Show percentage and label in each slice\n",
    "        textposition='inside',  # Show text inside the slices\n",
    "        marker=dict(line=dict(color='white', width=2)),  # Add a white border to slices\n",
    "    )\n",
    "\n",
    "    # Show the plot interactively\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Visualize class distribution as a pie chart in the training set with improved layout\n",
    "visualize_class_distribution_pie(train_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y2aTBwcLXTmz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817
    },
    "id": "y2aTBwcLXTmz",
    "outputId": "c8ba3431-f748-4baf-c4d4-2dae7a7bd85a"
   },
   "outputs": [],
   "source": [
    "# Distribution of the classes as the treemap\n",
    "def visualize_class_distribution_treemap(data_path):\n",
    "    # Get the list of entries (files and directories) in the data path\n",
    "    entries = os.listdir(data_path)\n",
    "\n",
    "    # Filter out non-directory entries (e.g., .DS_Store on macOS)\n",
    "    classes = [entry for entry in entries if os.path.isdir(os.path.join(data_path, entry))]\n",
    "\n",
    "    # Create a dictionary to store class distribution data\n",
    "    data = {'class': [], 'count': []}\n",
    "\n",
    "    for class_name in classes:\n",
    "        class_path = os.path.join(data_path, class_name)\n",
    "        num_samples = len(os.listdir(class_path))\n",
    "\n",
    "        data['class'].append(class_name)\n",
    "        data['count'].append(num_samples)\n",
    "\n",
    "    # Create a DataFrame from the data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Create an interactive treemap using Plotly Express\n",
    "    fig = px.treemap(df, path=['class'], values='count',\n",
    "                     color='count',  # Color the treemap based on the count\n",
    "                     hover_data={'count': True, 'class': False},  # Display count in hover information\n",
    "                     title='Class Distribution in Training Set using a Tree Map')\n",
    "\n",
    "    # Customize the layout for better appearance and increased figure size\n",
    "    fig.update_layout(\n",
    "        width=1000,  # Set the width of the figure\n",
    "        height=800,  # Set the height of the figure\n",
    "        margin=dict(l=20, r=20, t=40, b=20),  # Adjust margin for better spacing\n",
    "    )\n",
    "\n",
    "    # Show the plot interactively\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Visualize class distribution as a treemap in the training set with added features and increased figure size\n",
    "visualize_class_distribution_treemap(train_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7pMx5KUNeoEa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "7pMx5KUNeoEa",
    "outputId": "4f57021e-8721-4372-8e60-a8cbce1fe01a"
   },
   "outputs": [],
   "source": [
    "#Visualization of the distribution of the classes as the scatter plot.\n",
    "def visualize_class_distribution_scatter(data_path):\n",
    "    # Get the list of entries (files and directories) in the data path\n",
    "    entries = os.listdir(data_path)\n",
    "\n",
    "    # Filter out non-directory entries (e.g., .DS_Store on macOS)\n",
    "    classes = [entry for entry in entries if os.path.isdir(os.path.join(data_path, entry))]\n",
    "\n",
    "    # Create a dictionary to store class distribution data\n",
    "    data = {'class': [], 'count': []}\n",
    "\n",
    "    for class_name in classes:\n",
    "        class_path = os.path.join(data_path, class_name)\n",
    "        num_samples = len(os.listdir(class_path))\n",
    "\n",
    "        data['class'].append(class_name)\n",
    "        data['count'].append(num_samples)\n",
    "\n",
    "    # Create a DataFrame from the data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Create an interactive scatter plot using Plotly Express\n",
    "    fig = px.scatter(df, x='class', y='count', title='Class Distribution in Training Set using a Scatter Plot',\n",
    "                     labels={'class': 'Class', 'count': 'Number of Samples'},\n",
    "                     category_orders={'class': classes})  # Specify the order of categories on the x-axis\n",
    "\n",
    "    # Show the plot interactively\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "# Visualize class distribution as a scatter plot in the training set\n",
    "visualize_class_distribution_scatter(train_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a964ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have to increase more visualizations.\n",
    "# We can incldue image width Vs. Height\n",
    "# We can include image different aspect ration and the distribition of the image aspect ratios\n",
    "# We can use the label frequency as well as the line plot and everything\n",
    "# We can use different color contrast of the images as different plots maybe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OMUjg4LSYn_Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "OMUjg4LSYn_Y",
    "outputId": "600dff69-5eb8-4515-d2ea-975354ea593e"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ifIsVO0vkpHx",
   "metadata": {
    "id": "ifIsVO0vkpHx"
   },
   "source": [
    "# V. Data Pre-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ir_pKPpFyr",
   "metadata": {
    "id": "58ir_pKPpFyr"
   },
   "source": [
    "THe following are the steps which are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JiebU3Fckodl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JiebU3Fckodl",
    "outputId": "5fbbc3d7-4e9c-4406-8e40-92db453cba61"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to visualize each image with the class name, grayscale version, and dimensions\n",
    "def visualize_individual_samples_with_dimensions(data_path, num_samples_per_class=3, image_size=(64, 64)):\n",
    "    classes = os.listdir(data_path)\n",
    "\n",
    "    for class_name in classes:\n",
    "        class_path = os.path.join(data_path, class_name)\n",
    "        class_samples = os.listdir(class_path)[:num_samples_per_class]\n",
    "\n",
    "        for sample in class_samples:\n",
    "            sample_path = os.path.join(class_path, sample)\n",
    "\n",
    "            # Use OpenCV to read and convert the image to grayscale\n",
    "            original_img = cv2.imread(sample_path)\n",
    "            grayscale_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Get the dimensions of the original image\n",
    "            original_height, original_width, _ = original_img.shape\n",
    "\n",
    "            # Resize the images\n",
    "            original_img = cv2.resize(original_img, image_size)\n",
    "            grayscale_img = cv2.resize(grayscale_img, image_size)\n",
    "\n",
    "            # Plot the images with dimensions\n",
    "            plt.figure(figsize=(16, 6))\n",
    "\n",
    "            # Plot the original image on the left\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(f\"Original - Class: {class_name}\")\n",
    "            plt.axis('on')\n",
    "            plt.text(0.5, -0.1, f\"Dimensions: {original_height} x {original_width}\", ha='center', va='center', transform=plt.gca().transAxes)\n",
    "\n",
    "            # Plot the grayscale image in the middle\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(grayscale_img, cmap='gray')\n",
    "            plt.title(\"Grayscale\")\n",
    "            plt.axis('on')\n",
    "\n",
    "            # Plot the dimensions separately on the right\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.text(0.5, 0.5, f\"Dimensions: {image_size[0]} x {image_size[1]}\", ha='center', va='center', fontsize=14)\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "# Visualize each image from the training set with dimensions\n",
    "visualize_individual_samples_with_dimensions(train_path, image_size=(128, 128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sqyjH0zxkoaM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sqyjH0zxkoaM",
    "outputId": "a36f32c2-8e1b-443d-991c-510fba182f81"
   },
   "outputs": [],
   "source": [
    "# Function to visualize each image with the class name, edge-detected version, and dimensions\n",
    "def visualize_individual_samples_with_edge_detection(data_path, num_samples_per_class=3, image_size=(64, 64)):\n",
    "    classes = os.listdir(data_path)\n",
    "\n",
    "    for class_name in classes:\n",
    "        class_path = os.path.join(data_path, class_name)\n",
    "        class_samples = os.listdir(class_path)[:num_samples_per_class]\n",
    "\n",
    "        for sample in class_samples:\n",
    "            sample_path = os.path.join(class_path, sample)\n",
    "\n",
    "            # Use OpenCV to read and convert the image to grayscale\n",
    "            original_img = cv2.imread(sample_path)\n",
    "\n",
    "            # Apply Canny edge detection\n",
    "            edges_img = cv2.Canny(original_img, 50, 150)\n",
    "\n",
    "            # Get the dimensions of the original image\n",
    "            original_height, original_width, _ = original_img.shape\n",
    "\n",
    "            # Resize the images\n",
    "            original_img = cv2.resize(original_img, image_size)\n",
    "            edges_img = cv2.resize(edges_img, image_size)\n",
    "\n",
    "            # Create a figure with two subplots\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "            # Plot the original image on the left\n",
    "            axes[0].imshow(cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB))\n",
    "            axes[0].set_title(f\"Original - Class: {class_name}\")\n",
    "            axes[0].axis('off')\n",
    "\n",
    "            # Plot the edge-detected image in the middle\n",
    "            axes[1].imshow(edges_img, cmap='gray')\n",
    "            axes[1].set_title(\"Edge Detection\")\n",
    "            axes[1].axis('off')\n",
    "\n",
    "            # Show dimensions separately\n",
    "            dimensions_text = f\"Dimensions: {original_width} x {original_height}\"\n",
    "            axes[2].text(0.5, 0.5, dimensions_text, ha='center', va='center', fontsize=14)\n",
    "            axes[2].axis('off')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "# Visualize each image from the training set with edge detection and dimensions\n",
    "visualize_individual_samples_with_edge_detection(train_path, image_size=(128, 128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9734497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to save and plot the images which are normalised\n",
    "def normalize_images(train_path, test_path, save_train_path, save_test_path):\n",
    "    # Function to normalize a single image\n",
    "    def normalize_image(img):\n",
    "        return img / 255.0\n",
    "\n",
    "    def plot_images(original_img, normalized_img, dimensions_text):\n",
    "        # Create a figure with two subplots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        # Plot the original image on the left\n",
    "        axes[0].imshow(cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB))\n",
    "        axes[0].set_title(\"Original Image\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        # Plot the normalized image on the right\n",
    "        axes[1].imshow(normalized_img)\n",
    "        axes[1].set_title(\"Normalized Image\")\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        # Show dimensions separately\n",
    "        axes[1].text(0.5, 0.5, dimensions_text, ha='center', va='center', fontsize=12, color='white')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # Normalize and plot images for the training set\n",
    "    for class_name in os.listdir(train_path):\n",
    "        class_path = os.path.join(train_path, class_name)\n",
    "        save_class_path = os.path.join(save_train_path, class_name)\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(save_class_path):\n",
    "            os.makedirs(save_class_path)\n",
    "\n",
    "        for sample in os.listdir(class_path):\n",
    "            sample_path = os.path.join(class_path, sample)\n",
    "            save_sample_path = os.path.join(save_class_path, sample)\n",
    "\n",
    "            # Read the original image\n",
    "            original_img = cv2.imread(sample_path)\n",
    "\n",
    "            # Normalize the image\n",
    "            normalized_img = normalize_image(original_img)\n",
    "\n",
    "            # Save the normalized image\n",
    "            cv2.imwrite(save_sample_path, (normalized_img * 255).astype(np.uint8))\n",
    "\n",
    "            # Get dimensions of the original image\n",
    "            original_height, original_width, _ = original_img.shape\n",
    "            dimensions_text = f\"Dimensions: {original_width} x {original_height}\"\n",
    "\n",
    "            # Plot the images\n",
    "            plot_images(original_img, normalized_img, dimensions_text)\n",
    "\n",
    "    # Normalize and save images for the test set\n",
    "    for class_name in os.listdir(test_path):\n",
    "        class_path = os.path.join(test_path, class_name)\n",
    "        save_class_path = os.path.join(save_test_path, class_name)\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(save_class_path):\n",
    "            os.makedirs(save_class_path)\n",
    "\n",
    "        for sample in os.listdir(class_path):\n",
    "            sample_path = os.path.join(class_path, sample)\n",
    "            save_sample_path = os.path.join(save_class_path, sample)\n",
    "\n",
    "            # Read the original image\n",
    "            original_img = cv2.imread(sample_path)\n",
    "\n",
    "            # Normalize the image\n",
    "            normalized_img = normalize_image(original_img)\n",
    "\n",
    "            # Save the normalized image\n",
    "            cv2.imwrite(save_sample_path, (normalized_img * 255).astype(np.uint8))\n",
    "\n",
    "# Specify paths for the paths for saving normalized images\n",
    "save_train_path = \"/Users/sarveswararaopatchipulusu/Desktop/advanced python class/archive 2/normalisedtrain\"\n",
    "save_test_path = \"/Users/sarveswararaopatchipulusu/Desktop/advanced python class/archive 2/normalisedtest\"\n",
    "\n",
    "# Call the function to normalize and plot images\n",
    "normalize_images(train_path, test_path, save_train_path, save_test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d94619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_images(original_img, adjusted_img, dimensions_text):\n",
    "    # Create a figure with two subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Plot the original image on the left\n",
    "    axes[0].imshow(cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Plot the adjusted image on the right\n",
    "    axes[1].imshow(cv2.cvtColor(adjusted_img, cv2.COLOR_BGR2RGB))\n",
    "    axes[1].set_title(\"Adjusted Image\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # Show dimensions separately\n",
    "    axes[1].text(0.5, 0.5, dimensions_text, ha='center', va='center', fontsize=12, color='white')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Function to adjust the contrast.\n",
    "def contrast_adjustment_and_save(data_path, save_path, alpha=1.5, beta=25):\n",
    "    # Function to adjust contrast of a single image\n",
    "    def adjust_contrast(img, alpha, beta):\n",
    "        adjusted_img = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
    "        return adjusted_img\n",
    "\n",
    "    # Apply contrast adjustment and save images\n",
    "    for class_name in os.listdir(data_path):\n",
    "        class_path = os.path.join(data_path, class_name)\n",
    "        save_class_path = os.path.join(save_path, class_name)\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(save_class_path):\n",
    "            os.makedirs(save_class_path)\n",
    "\n",
    "        for sample in os.listdir(class_path):\n",
    "            sample_path = os.path.join(class_path, sample)\n",
    "            save_sample_path = os.path.join(save_class_path, sample)\n",
    "\n",
    "            # Read the original image\n",
    "            original_img = cv2.imread(sample_path)\n",
    "\n",
    "            # Adjust contrast\n",
    "            adjusted_img = adjust_contrast(original_img, alpha, beta)\n",
    "\n",
    "            # Save the adjusted image\n",
    "            cv2.imwrite(save_sample_path, adjusted_img)\n",
    "\n",
    "            # Get dimensions of the original image\n",
    "            original_height, original_width, _ = original_img.shape\n",
    "            dimensions_text = f\"Dimensions: {original_width} x {original_height}\"\n",
    "\n",
    "            # Plot the images\n",
    "            plot_images(original_img, adjusted_img, dimensions_text)\n",
    "\n",
    "# Specify paths for the data and the path for saving contrast-adjusted images\n",
    "train_path = r'/Users/sarveswararaopatchipulusu/Desktop/advanced python class/archive 2/train'\n",
    "save_path_contrast = \"/Users/sarveswararaopatchipulusu/Desktop/advanced python class/archive 2/contrast_adjusted_train\"\n",
    "\n",
    "# Call the function to adjust contrast and save images\n",
    "contrast_adjustment_and_save(train_path, save_path_contrast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o9Rb3QT7koNe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "o9Rb3QT7koNe",
    "outputId": "9f11d6dc-7d46-4bed-9f10-c62bed715cc1"
   },
   "outputs": [],
   "source": [
    "\n",
    "#This is not working!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1\n",
    "\n",
    "# Replace 'YOUR_API_KEY' with the actual API key from OCR.space\n",
    "OCR_API_KEY = 'K83379042788957'\n",
    "\n",
    "# Function to perform OCR using OCR.space API\n",
    "def perform_ocr(image):\n",
    "    # Convert the image to bytes\n",
    "    img_byte_array = cv2.imencode('.png', image)[1].tobytes()\n",
    "\n",
    "    # Make a POST request to OCR.space API\n",
    "    response = requests.post(\n",
    "        url='https://api.ocr.space/parse/image',\n",
    "        headers={\n",
    "            'apikey': OCR_API_KEY,\n",
    "            'Content-Type': 'application/json',\n",
    "        },\n",
    "        files={'file': ('image.png', img_byte_array)}\n",
    "    )\n",
    "\n",
    "    # Parse the OCR result\n",
    "    ocr_result = response.json()\n",
    "\n",
    "    # Check if there are parsed results\n",
    "    parsed_results = ocr_result.get('ParsedResults', [])\n",
    "    if not parsed_results:\n",
    "        return \"No OCR result\"\n",
    "\n",
    "    # Get the first parsed result (assuming there is at least one)\n",
    "    first_result = parsed_results[0]\n",
    "\n",
    "    # Get the parsed text\n",
    "    ocr_text = first_result.get('ParsedText', '')\n",
    "\n",
    "    return ocr_text\n",
    "\n",
    "# Function to visualize each image with the class name, OCR text, and dimensions\n",
    "def visualize_individual_samples_with_ocr(data_path, num_samples_per_class=3, image_size=(64, 64)):\n",
    "    classes = os.listdir(data_path)\n",
    "\n",
    "    for class_name in classes:\n",
    "        class_path = os.path.join(data_path, class_name)\n",
    "        class_samples = os.listdir(class_path)[:num_samples_per_class]\n",
    "\n",
    "        for sample in class_samples:\n",
    "            sample_path = os.path.join(class_path, sample)\n",
    "\n",
    "            # Use OpenCV to read and convert the image to grayscale\n",
    "            original_img = cv2.imread(sample_path)\n",
    "\n",
    "            # Perform OCR on the original image\n",
    "            ocr_text = perform_ocr(original_img)\n",
    "\n",
    "            # Get the dimensions of the original image\n",
    "            original_height, original_width, _ = original_img.shape\n",
    "\n",
    "            # Resize the original image\n",
    "            original_img = cv2.resize(original_img, image_size)\n",
    "\n",
    "            # Create a figure with two subplots\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "            # Plot the original image on the left\n",
    "            axes[0].imshow(cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB))\n",
    "            axes[0].set_title(f\"Original - Class: {class_name}\")\n",
    "            axes[0].axis('on')\n",
    "\n",
    "            # Display OCR text and dimensions on the right\n",
    "            text_and_dimensions = f\"OCR Text:\\n{ocr_text}\\n\\nDimensions: {original_width} x {original_height}\"\n",
    "            axes[1].text(0.5, 0.5, text_and_dimensions, ha='center', va='center', fontsize=12)\n",
    "            axes[1].axis('on')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "# Visualize each image from the training set with OCR text and dimensions\n",
    "visualize_individual_samples_with_ocr(train_path, image_size=(128, 128))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wI2UJyHFrUQx",
   "metadata": {
    "id": "wI2UJyHFrUQx"
   },
   "source": [
    "# VI. Model Building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9f7e2",
   "metadata": {},
   "source": [
    "# Model 1 - A :  CNN - Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b55806f",
   "metadata": {},
   "source": [
    "The first model is CNN which is convolution neural networks. And the code for that is as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb96d1",
   "metadata": {},
   "source": [
    "** On training Set and Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_dQh9QIKkn3B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_dQh9QIKkn3B",
    "outputId": "cdffe561-6ce9-4ac2-cb6c-a67e7cddba66"
   },
   "outputs": [],
   "source": [
    "dataset_path = train_path\n",
    "\n",
    "# Define the image size and batch size\n",
    "image_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Use ImageDataGenerator for data augmentation and normalization\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # 80% for training, 20% for validation\n",
    ")\n",
    "\n",
    "# Load and split the dataset into training and validation sets\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Assuming you have multiple classes\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))  # Optional dropout layer for regularization\n",
    "model.add(layers.Dense(100, activation='softmax'))  # Assuming you have 100 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,  # You can adjust the number of epochs based on your dataset and training performance\n",
    "    validation_data=validation_generator\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('cnn_model.h5')\n",
    "\n",
    "# Evaluate the model on the test set if available\n",
    "# test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# test_generator = test_datagen.flow_from_directory(test_path, target_size=image_size, batch_size=batch_size, class_mode='categorical')\n",
    "# test_loss, test_acc = model.evaluate(test_generator)\n",
    "# print(f'Test Accuracy: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24acfa88",
   "metadata": {},
   "source": [
    "Plotting the results with matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050ea42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a variable 'history' that contains the training history\n",
    "history = {\n",
    "    'loss': [4.4792, 4.0002, 3.6973, 3.4732, 3.2785, 3.1288, 3.0180, 2.9181, 2.8353, 2.7545],\n",
    "    'accuracy': [0.0306, 0.0784, 0.1216, 0.1662, 0.1912, 0.2157, 0.2387, 0.2565, 0.2740, 0.2900],\n",
    "    'val_loss': [4.1028, 3.6681, 3.4190, 3.1808, 3.0134, 2.8856, 2.8474, 2.7490, 2.6982, 2.7008],\n",
    "    'val_accuracy': [0.0775, 0.1362, 0.1810, 0.2344, 0.2592, 0.2904, 0.2942, 0.3119, 0.3277, 0.3160]\n",
    "}\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['loss'], label='Training Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc60484",
   "metadata": {},
   "source": [
    "Plotting the results with Plotly Express."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Assuming you have a variable 'history' that contains the training history\n",
    "history = {\n",
    "    'loss': [4.4792, 4.0002, 3.6973, 3.4732, 3.2785, 3.1288, 3.0180, 2.9181, 2.8353, 2.7545],\n",
    "    'accuracy': [0.0306, 0.0784, 0.1216, 0.1662, 0.1912, 0.2157, 0.2387, 0.2565, 0.2740, 0.2900],\n",
    "    'val_loss': [4.1028, 3.6681, 3.4190, 3.1808, 3.0134, 2.8856, 2.8474, 2.7490, 2.6982, 2.7008],\n",
    "    'val_accuracy': [0.0775, 0.1362, 0.1810, 0.2344, 0.2592, 0.2904, 0.2942, 0.3119, 0.3277, 0.3160]\n",
    "}\n",
    "\n",
    "# Create a DataFrame for the training history\n",
    "df_history = pd.DataFrame(history)\n",
    "epochs = list(range(1, len(history['loss']) + 1))\n",
    "df_history['Epoch'] = epochs\n",
    "\n",
    "# Create line charts using Plotly Express\n",
    "fig_loss = px.line(df_history, x='Epoch', y=['loss', 'val_loss'], labels={'value': 'Loss'},\n",
    "                   title='Training and Validation Loss')\n",
    "fig_accuracy = px.line(df_history, x='Epoch', y=['accuracy', 'val_accuracy'], labels={'value': 'Accuracy'},\n",
    "                       title='Training and Validation Accuracy')\n",
    "\n",
    "# Show the plots interactively\n",
    "fig_loss.show()\n",
    "fig_accuracy.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717ec829",
   "metadata": {},
   "source": [
    "Evaluation of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8efffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set.\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(test_path, target_size=image_size, batch_size=batch_size, class_mode='categorical')\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f'Test Accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c51a00",
   "metadata": {},
   "source": [
    "Plotting the model accuracy with plotly express"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e11d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Assuming you have the test accuracy value\n",
    "test_accuracy = 0.3840000033378601\n",
    "\n",
    "# Create a DataFrame for the test accuracy\n",
    "df_test_accuracy = pd.DataFrame({'Metric': ['Test Accuracy'], 'Value': [test_accuracy]})\n",
    "\n",
    "# Create a bar chart using Plotly Express\n",
    "fig_test_accuracy = px.bar(df_test_accuracy, x='Metric', y='Value', text='Value',\n",
    "                           title='Test Accuracy',\n",
    "                           labels={'Metric': 'Metric', 'Value': 'Value'})\n",
    "\n",
    "# Customize the layout for better appearance\n",
    "fig_test_accuracy.update_layout(\n",
    "    yaxis=dict(title='Accuracy'),\n",
    "    showlegend=False,\n",
    "    height=400,\n",
    "    width=500\n",
    ")\n",
    "\n",
    "# Show the plot interactively\n",
    "fig_test_accuracy.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f24a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming you have the test_generator and model\n",
    "# test_generator = test_datagen.flow_from_directory(test_path, target_size=image_size, batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = test_generator.classes\n",
    "\n",
    "# Calculate confusion matrix for each class separately\n",
    "class_names = list(test_generator.class_indices.keys())\n",
    "for i in range(len(class_names)):\n",
    "    class_name = class_names[i]\n",
    "    \n",
    "    class_true_labels = [1 if true_label == i else 0 for true_label in true_labels]\n",
    "    class_predicted_labels = [1 if predicted_label == i else 0 for predicted_label in predicted_labels]\n",
    "    \n",
    "    conf_mat = confusion_matrix(class_true_labels, class_predicted_labels)\n",
    "    \n",
    "    # Plot confusion matrix using seaborn\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=['Not ' + class_name, class_name],\n",
    "                yticklabels=['Not ' + class_name, class_name])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix for Class: {class_name}')\n",
    "    plt.show()\n",
    "\n",
    "# Display sample images with true and predicted labels\n",
    "num_samples = 5\n",
    "sample_indices = np.random.choice(len(test_generator.filepaths), num_samples, replace=False)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    img_path = test_generator.filepaths[idx]\n",
    "    img = plt.imread(img_path)\n",
    "    true_label = test_generator.classes[idx]\n",
    "    predicted_label = predicted_labels[idx]\n",
    "\n",
    "    plt.subplot(1, num_samples, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'True: {test_generator.class_indices.keys()} | Predicted: {test_generator.class_indices.keys()}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e437bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Assuming you have the test_generator and model\n",
    "# test_generator = test_datagen.flow_from_directory(test_path, target_size=image_size, batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = test_generator.classes\n",
    "\n",
    "# Map class indices to class labels\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Display all images with true and predicted labels\n",
    "num_samples = len(test_generator.filepaths)\n",
    "num_columns = 5  # You can adjust the number of columns\n",
    "\n",
    "num_rows = (num_samples + num_columns - 1) // num_columns\n",
    "plt.figure(figsize=(15, 3 * num_rows))\n",
    "\n",
    "gs = gridspec.GridSpec(num_rows, num_columns, wspace=0.05, hspace=0.3)\n",
    "\n",
    "for i, idx in enumerate(range(num_samples)):\n",
    "    img_path = test_generator.filepaths[idx]\n",
    "    img = plt.imread(img_path)\n",
    "    true_label = class_labels[true_labels[idx]]\n",
    "    predicted_label = class_labels[predicted_labels[idx]]\n",
    "\n",
    "    ax = plt.subplot(gs[i])\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'T: {true_label}\\nP: {predicted_label}', fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f8f6e3",
   "metadata": {},
   "source": [
    "# Model 1 - B: CNN Using Hyper-paramter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aa43bd",
   "metadata": {},
   "source": [
    "Hyper parameter tuning in CNN.\n",
    "\n",
    "Number of Filters and Kernel Size for Convolutional Layers:\n",
    "\n",
    "Tuned Parameters:\n",
    "conv1_filters: Number of filters in the first convolutional layer (tuned between 32, 64, 96, 128).\n",
    "conv1_kernel: Kernel size for the first convolutional layer (tuned between 3x3 and 5x5).\n",
    "conv2_filters: Number of filters in the second convolutional layer (tuned between 64, 128, 192, 256).\n",
    "conv2_kernel: Kernel size for the second convolutional layer (tuned between 3x3 and 5x5).\n",
    "\n",
    "Reasoning:\n",
    "The number of filters in convolutional layers determines the complexity and expressiveness of the learned features.\n",
    "The kernel size influences the receptive field and the type of features captured by each filter.\n",
    "\n",
    "Number of Neurons in Dense (Fully Connected) Layer:\n",
    "\n",
    "Tuned Parameter:\n",
    "dense_units: Number of neurons in the dense layer (tuned between 128, 256, 384, 512).\n",
    "\n",
    "Reasoning:\n",
    "The number of neurons in the dense layer affects the capacity of the network to capture high-level abstractions.\n",
    "\n",
    "Learning Rate:\n",
    "\n",
    "Tuned Parameter:\n",
    "learning_rate: Learning rate for the Adam optimizer (tuned between 1e-2, 1e-3, 1e-4).\n",
    "\n",
    "Reasoning:\n",
    "Learning rate controls the step size during optimization and influences the convergence and stability of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534e8b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f23e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions of input images\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "img_channels = 3  # Assuming RGB images\n",
    "\n",
    "# Define the number of classes in your dataset\n",
    "num_classes = 100  # Adjust based on your dataset\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(\n",
    "        filters=hp.Int('conv1_filters', min_value=32, max_value=128, step=32),\n",
    "        kernel_size=hp.Choice('conv1_kernel', values=[3, 5]),\n",
    "        activation='relu',\n",
    "        input_shape=(img_height, img_width, img_channels)\n",
    "    ))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Conv2D(\n",
    "        filters=hp.Int('conv2_filters', min_value=64, max_value=256, step=64),\n",
    "        kernel_size=hp.Choice('conv2_kernel', values=[3, 5]),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(\n",
    "        units=hp.Int('dense_units', min_value=128, max_value=512, step=128),\n",
    "        activation='relu'\n",
    "    ))\n",
    "\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Instantiate the tuner\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    directory='my_dir',\n",
    "    project_name='helloworld'\n",
    ")\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "tuner.search(train_generator, epochs=10, validation_data=validation_generator)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Build the final model with the best hyperparameters\n",
    "final_model = tuner.hypermodel.build(best_hps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ada83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04550aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d9e15a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6bd86a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53686043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e91e5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f07fb86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd86df6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb297269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add17b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f52622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec0c3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86bffeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4057d104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a75590d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d7ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2da6a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486431b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac8dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c073f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac52cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b00c82a8",
   "metadata": {},
   "source": [
    "We have plotted the results we have got in the CNN model using plotly express."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a960c",
   "metadata": {
    "id": "809a960c"
   },
   "source": [
    "# 1. Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d223d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f20305",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "50f20305",
    "outputId": "a6e2446d-ad28-491f-cda4-edae339c2780",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Read an image using OpenCV\n",
    "image_path = 'train/basketball/001.jpg'\n",
    "original_image = cv2.imread(image_path,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(original_image),plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ebe6a0",
   "metadata": {
    "id": "52ebe6a0"
   },
   "source": [
    "Image is blue because OpenCV by default print an image in BGR format while Matplotlib does in RGB format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6439969",
   "metadata": {
    "id": "c6439969"
   },
   "source": [
    "# 2. Resize image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeff96d",
   "metadata": {
    "id": "5aeff96d"
   },
   "source": [
    "# 3. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DCrIfholbjdG",
   "metadata": {
    "id": "DCrIfholbjdG"
   },
   "source": [
    "Normalizing imagee for thw whole trainset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EZigiwzcbbkH",
   "metadata": {
    "id": "EZigiwzcbbkH"
   },
   "outputs": [],
   "source": [
    "\n",
    "def normalize_image(image):\n",
    "    # Convert image to floating-point format\n",
    "    normalized_image = image.astype('float32')\n",
    "\n",
    "    # Normalize pixel values to the range [0, 1]\n",
    "    normalized_image /= 255.0\n",
    "\n",
    "    return normalized_image\n",
    "\n",
    "def process_images_in_directory(directory_path):\n",
    "    # List all files in the directory\n",
    "    file_list = os.listdir(directory_path)\n",
    "\n",
    "    for file_name in file_list:\n",
    "        # Construct the full path to the image file\n",
    "        image_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "        try:\n",
    "            # Read the image\n",
    "            original_image = cv2.imread(image_path)\n",
    "\n",
    "            # Ensure the image is not None\n",
    "            if original_image is not None:\n",
    "                # Normalize the image\n",
    "                normalized_image = normalize_image(original_image)\n",
    "\n",
    "                # Display the original and normalized images (optional)\n",
    "                cv2.imshow('Original Image', original_image)\n",
    "                cv2.imshow('Normalized Image', normalized_image)\n",
    "                cv2.waitKey(0)\n",
    "                cv2.destroyAllWindows()\n",
    "\n",
    "                # Save the normalized image back to the directory (optional)\n",
    "                normalized_image_path = os.path.join(directory_path, f'normalized_{file_name}')\n",
    "                cv2.imwrite(normalized_image_path, (normalized_image * 255).astype(np.uint8))\n",
    "            else:\n",
    "                print(f\"Error: Unable to read the image from {image_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path}: {str(e)}\")\n",
    "\n",
    "# Example: Process images in the training directory\n",
    "train_directory = '/content/dataset/train'\n",
    "process_images_in_directory(train_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X_JJstdqbcF8",
   "metadata": {
    "id": "X_JJstdqbcF8"
   },
   "source": [
    "This is for single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b81b75e",
   "metadata": {
    "id": "5b81b75e"
   },
   "outputs": [],
   "source": [
    "def normalize_image(image):\n",
    "    # Convert image to floating-point format\n",
    "    normalized_image = image.astype('float32')\n",
    "\n",
    "    # Normalize pixel values to the range [0, 1]\n",
    "    normalized_image /= 255.0\n",
    "\n",
    "    return normalized_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a831ffbc",
   "metadata": {
    "id": "a831ffbc"
   },
   "outputs": [],
   "source": [
    "# Ensure the image is not None\n",
    "if original_image is not None:\n",
    "    # Normalize the image\n",
    "    normalized_image = normalize_image(original_image)\n",
    "\n",
    "    # Display the original and normalized images (optional)\n",
    "    cv2.imshow('Original Image', original_image)\n",
    "    cv2.imshow('Normalized Image', normalized_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    print(f\"Error: Unable to read the image from {image_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397d25ec",
   "metadata": {
    "id": "397d25ec"
   },
   "source": [
    "Check normalized or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8116023",
   "metadata": {
    "id": "b8116023"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ensure the image is not None\n",
    "if original_image is not None:\n",
    "\n",
    "    height, width, channels = original_image.shape\n",
    "    print(f\"Image shape: {height} x {width} x {channels}\")\n",
    "\n",
    "    # Choose a pixel coordinate (x, y)\n",
    "    x, y = 100, 50  # Example coordinates, adjust as needed\n",
    "\n",
    "    # Print pixel values at the chosen coordinate\n",
    "    pixel_values = original_image[y, x]\n",
    "    normalized_pixel_values = normalized_image[y, x]\n",
    "    print(f\"Pixel values at ({x}, {y}): {pixel_values}\")\n",
    "    print(f\"Normalized Pixel values at ({x}, {y}): {normalized_pixel_values}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error: Unable to read the image from {image_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b537b759",
   "metadata": {
    "id": "b537b759"
   },
   "source": [
    "# 4. Color Adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f934061f",
   "metadata": {
    "id": "f934061f"
   },
   "source": [
    "Make sure image is in RGB or required color space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ffb605",
   "metadata": {
    "id": "21ffb605"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ensure the image is not None\n",
    "if original_image is not None:\n",
    "    # Convert BGR to RGB\n",
    "    rgb_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Display the RGB image using Matplotlib\n",
    "    plt.imshow(rgb_image)\n",
    "    plt.title('RGB Image')\n",
    "    plt.axis('off')  # Optional: Turn off axis labels\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Error: Unable to read the image from {image_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f7372d",
   "metadata": {
    "id": "d7f7372d"
   },
   "source": [
    "# 5. Grayscale Adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6612874",
   "metadata": {
    "id": "c6612874"
   },
   "source": [
    "# 6. Contrast Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e8eafd",
   "metadata": {
    "id": "65e8eafd"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Ensure the image is not None\n",
    "if original_image is not None:\n",
    "    # Split the image into individual color channels\n",
    "    b, g, r = cv2.split(rgb_image)\n",
    "\n",
    "    # Adjust contrast for each channel (adjust the contrast factor as needed)\n",
    "    contrast_factor = 1.2\n",
    "    adjusted_b = cv2.convertScaleAbs(b, alpha=contrast_factor, beta=0)\n",
    "    adjusted_g = cv2.convertScaleAbs(g, alpha=contrast_factor, beta=0)\n",
    "    adjusted_r = cv2.convertScaleAbs(r, alpha=contrast_factor, beta=0)\n",
    "\n",
    "    # Merge the adjusted channels back into an RGB image\n",
    "    contrast_adjusted_image = cv2.merge([adjusted_b, adjusted_g, adjusted_r])\n",
    "\n",
    "    # Display the original and contrast-adjusted images\n",
    "    plt.imshow(rgb_image)\n",
    "    plt.show()\n",
    "    plt.imshow(contrast_adjusted_image)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Error: Unable to read the image from {image_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1fbe41",
   "metadata": {
    "id": "0e1fbe41"
   },
   "source": [
    "# 7. Image Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07baddce",
   "metadata": {
    "id": "07baddce"
   },
   "source": [
    "using another folder containing 3 random images from different folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eee21db",
   "metadata": {
    "id": "1eee21db"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd952ab",
   "metadata": {
    "id": "7cd952ab"
   },
   "outputs": [],
   "source": [
    "# Define the input and output directories\n",
    "input_dir = 'Example dataset'\n",
    "output_dir = 'augmented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fccd71",
   "metadata": {
    "id": "13fccd71"
   },
   "outputs": [],
   "source": [
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Initialize the ImageDataGenerator with desired augmentation parameters\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5aa026",
   "metadata": {
    "id": "ec5aa026"
   },
   "outputs": [],
   "source": [
    "# Loop through all images in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    input_path = os.path.join(input_dir, filename)\n",
    "\n",
    "    # Read the image\n",
    "    img = Image.open(input_path)\n",
    "    img_array = np.array(img)\n",
    "\n",
    "    # Reshape the image to (1, height, width, channels) for flow method\n",
    "    img_array = img_array.reshape((1,) + img_array.shape)\n",
    "\n",
    "    # Generate augmented images\n",
    "    i = 0\n",
    "    for batch in datagen.flow(img_array, batch_size=1, save_to_dir=output_dir, save_prefix='aug', save_format='jpg'):\n",
    "        i += 1\n",
    "        if i >= 5:  # Generate 5 augmented images per original image\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16327442",
   "metadata": {
    "id": "16327442"
   },
   "source": [
    "basically increases dataset size, good for reducing overfitting and generalization. Not sure if we should implement it or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38803a00",
   "metadata": {
    "id": "38803a00"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
